{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accepting-value",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rocky-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3]\n",
    "myarr = np.array(mylist)\n",
    "myarr = np.array([1, 2, 3])\n",
    "\n",
    "my_matrix = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "np.array(my_matrix)\n",
    "np.array([[1,2,3], [4,5,6],[7,8,9]])\n",
    "\n",
    "np.arange(0, 10)\n",
    "\n",
    "np.arange(0 , 11, 2) \n",
    "np.arange(30, 0, -1)\n",
    "\n",
    "np.zeros(10)\n",
    "\n",
    "np.zeros((3,3))\n",
    "\n",
    "np.zeros((3,3), dtype=bool)\n",
    "\n",
    "np.ones(10, dtype=int)\n",
    "\n",
    "np.ones((10,10), dtype=bool)\n",
    "\n",
    "np.full((3,5),3)\n",
    "np.full((4,4), \"3\")\n",
    "\n",
    "np.linspace(0, 10, 3)  # 0 to 10 with 3 steps\n",
    "\n",
    "np.linspace(0,10)  # default 50 steps\n",
    "\n",
    "np.linspace(0, 50, 10, dtype='int')\n",
    "\n",
    "np.eye(5)\n",
    "\n",
    "np.random.rand(10) # 10 random numbers, each 0 to 1  \n",
    "\n",
    "np.random.rand(5,5)\n",
    "\n",
    "pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(np.random.rand(10000),bins=40)\n",
    "\n",
    "np.random.randn(10) # numbers which mean=0\n",
    "\n",
    "np.random.randn(5,5)\n",
    "\n",
    "np.random.randn(10000).mean() --> 0.0000123\n",
    "\n",
    "np.random.randint(0, 100, 10)  # random 10 numbers between 0 to 100 \n",
    "\n",
    "np.random.randint(10, size=10) # random 10 numbers (size 100 to 10\n",
    "\n",
    "a.shape\n",
    "\n",
    "a.sort()\n",
    "np.sort(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-enlargement",
   "metadata": {},
   "source": [
    "# SERIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "correct-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "myser = pd.Series(data = mydata, index=myindex)\n",
    "\n",
    "ages = {'Sam':5, 'Frank':10, 'Spike':7}\n",
    "pd.Series(ages)\n",
    "\n",
    "q1 = {'Japan': 80, 'China': 450, 'India': 200, 'USA': 250}\n",
    "q2 = {'Brazil': 100,'China': 500, 'India': 210,'USA': 260}\n",
    "sales_q1 = pd.Series(q1)\n",
    "sales_q2 = pd.Series(q2)\n",
    "\n",
    "sales_q1['Japan']\n",
    "\n",
    "sales_q1[0]\n",
    "\n",
    "sales_q1.keys()\n",
    "\n",
    "sales_q1 * 2\n",
    "\n",
    "sales_q1 / 100\n",
    "\n",
    "sales_q1 + sales_q2\n",
    "\n",
    "sales_q1.add(sales_q2, fill_value = 0) # float\n",
    "\n",
    "sales_q1.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-stewart",
   "metadata": {},
   "source": [
    "# DATA FRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-sacrifice",
   "metadata": {},
   "source": [
    "### creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "mydata = np.random.randint(0,101,(4,3))\n",
    "myindex = ['CA', 'NY', 'AZ', 'TX']\n",
    "mycolumns = ['Jan', 'Fab', 'Mar']\n",
    "df = pd.DataFrame(data = mydata, index = myindex, columns = mycolumns)\n",
    "\n",
    "df = pd.read_csv('tips.csv')\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\hÃ¼seyin\\\\tips.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset(\"tips\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-nigeria",
   "metadata": {},
   "source": [
    "### basic properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    ".info()\n",
    "\n",
    ".columns\n",
    "\n",
    ".index\n",
    "\n",
    ".shape\n",
    "\n",
    ".head()  / df.head(10)\n",
    "\n",
    ".tail()  / df.tail(2)\n",
    "\n",
    ".ndim()\n",
    "\n",
    ".get_dummies()\n",
    "\n",
    ".groupby()\n",
    "\n",
    ".size()\n",
    "\n",
    ".values()\n",
    "\n",
    ".loc()\n",
    "\n",
    ".iloc()\n",
    "\n",
    ".xs()\n",
    "\n",
    ".drop()\n",
    "\n",
    ".dropna()\n",
    "\n",
    ".join()\n",
    "\n",
    ".merge()\n",
    "\n",
    ".concat()\n",
    "\n",
    ".describe()\n",
    "\n",
    ".describe().\n",
    "\n",
    ".transpose()  / .T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-andrew",
   "metadata": {},
   "source": [
    "### working with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_bill']\n",
    "\n",
    "mycols = ['total_bill', 'tip'] / df[['total_bill', 'tip']]\n",
    "\n",
    "df['tip'] + df['total_bill'] \n",
    "\n",
    "df['tip'] / df['total_bill']\n",
    "\n",
    "# CREATE NEW COLUMN:\n",
    "\n",
    "df['tip_percentage'] = 100 * df['tip'] / df['total_bill'], inplace = True\n",
    "\n",
    "df['price_per_person'] = df['total_bill'] / df['size']\n",
    "\n",
    "np.round(df['total_bill'] / df['size'], 2)\n",
    "\n",
    "df['price_per_person'] = np.round(df['total_bill'] / df['size'], 2)\n",
    "\n",
    "# DROP COLUMN\n",
    "\n",
    "df.drop('tip_percentage', axis = 1) \n",
    "\n",
    "df = df.drop('tip_percentage', axis = 1)  / df.drop('tip_percentage', axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-restaurant",
   "metadata": {},
   "source": [
    "### working with rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index\n",
    "\n",
    "df.set_index(\"Payment ID\")\n",
    "\n",
    "df.reset_index()\n",
    "\n",
    "df.iloc[0]\n",
    "\n",
    "df.iloc[0:4]\n",
    "\n",
    "df.loc['Sun2959']\n",
    "\n",
    "df.loc[['Sun2959', 'Sun5260']]\n",
    "\n",
    "df.drop('Sun2959', axis = 0)\n",
    "\n",
    "one_row = df.iloc[0]\n",
    "df = df.append(one_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-visit",
   "metadata": {},
   "source": [
    "### conditional filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_bill'] > 40\n",
    "\n",
    "type(df['total_bill'] > 40)  # series\n",
    "\n",
    "bool_series = df['total_bill'] > 40\n",
    "df[bool_series]  ==  df[df['total_bill'] > 40]\n",
    "\n",
    "df[df['sex'] == \"Male\"]\n",
    "\n",
    "df[df['size'] > 3]\n",
    "\n",
    "df[(df['total_bill'] > 30) & (df['sex'] == \"Male\")]  # and\n",
    "\n",
    "df[(df['day'] == 'Sun') | (df['day'] == \"Sat\")]  # or\n",
    "\n",
    "options = ['Sat', 'Sun']\n",
    "\n",
    ".isin\n",
    "\n",
    "df['day'].isin(options)  # boolean\n",
    "df[df['day'].isin(options)]  # dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-plasma",
   "metadata": {},
   "source": [
    "### usefull methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_four(num) :\n",
    "    return str(num)[-4:]\n",
    "last_four(123456789) # 6789\n",
    "df['CC Number'].apply(last_four)\n",
    "df['last_four'] = df['CC Number'].apply(last_four) # new column\n",
    "\n",
    ".mean()\n",
    "\n",
    "df['total_bill'].mean()\n",
    "\n",
    ".apply()\n",
    "\n",
    "def simple(num) :\n",
    "    return num * 2\n",
    "df['total_bill'].apply(simple)\n",
    "\n",
    "df['total_bill'].apply(lambda num : num * 2)\n",
    "\n",
    ".apply(lambda......)\n",
    "\n",
    "def quality(total_bill, tip) :\n",
    "    if tip / total_bill > 0.25:\n",
    "        return \"Generous\"\n",
    "    else : \n",
    "        return \"Other\"\n",
    "df[['total_bill', 'tip']].apply(lambda df : quality(df['total_bill'], df['tip']), axis = 1)\n",
    "df['Quality'] = df[['total_bill', 'tip']].apply(lambda df : quality(df['total_bill'], df['tip']), axis = 1)\n",
    "\n",
    "df['Quality'] = np.vectorize(quality)(df['total_bill'], df['tip'])  # the same result with \"vectorize\" method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-moore",
   "metadata": {},
   "source": [
    "### Usefull methods-Statistical Information and Sorting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interpreted-cocktail",
   "metadata": {},
   "source": [
    ".max()       .idxmax()     .min()     .idxmin()\n",
    "\n",
    ".iloc[]      .corr()      .unique()    .value_counts()\n",
    "\n",
    ".replace()   .map()    .dublicated()   .drop_dublicates()\n",
    "\n",
    ".between()   .nlargest()   .sort_values   .sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('tip', ascending = False)  # default--> True\n",
    "\n",
    "df.sort_values(['tip','size'])\n",
    "\n",
    "df['total_bill'].max()\n",
    "\n",
    "df['total_bill'].idxmax()  # --> 170\n",
    "df.iloc[170]  # see the values of the row 170\n",
    "\n",
    "df['total_bill'].min()\n",
    "\n",
    "df['total_bill'].idxmin()\n",
    "df.iloc[67]  /  df.iloc[df['total_bill'].idxmin()]\n",
    "\n",
    "df.corr()\n",
    "\n",
    "df['day'].unique()   # len(df['day'].unique())\n",
    "\n",
    "df['day'].value_counts()\n",
    "\n",
    "df['sex'].replace(['Female', 'Male'], ['F', 'M'])\n",
    "\n",
    "mymap = {'Female': 'F', 'Male': 'M'}\n",
    "df['sex'].map(mymap)   # the same result with \"map\" method\n",
    "\n",
    "df.duplicated()  # boolean\n",
    "\n",
    "df.drop_duplicates()\n",
    "\n",
    "df['total_bill'].between(10, 20, inclusive = True)  # boolean\n",
    "\n",
    "df.nlargest(5, 'tip')  # Return the first `n` rows ordered by `columns` in descending order\n",
    "\n",
    "df.sort_values('tip', ascending = False).iloc[0:5]\n",
    "\n",
    "df.sample(5)\n",
    "\n",
    "df.sample(frac = 0.1) # Fraction of axis items to return. Cannot be used with `n`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-diploma",
   "metadata": {},
   "source": [
    "### groupby operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model_year'].unique()  # see the unique of model years\n",
    "df['model_year'].value_counts()  # see the count of the cars by their model year\n",
    "\n",
    "df.groupby('model_year')  # still waiting to perform future calculations from the aggregate method.\n",
    "\n",
    "df.groupby('model_year').mean()  # now model_year is an index label\n",
    "\n",
    "df.groupby('model_year').mean()['mpg']\n",
    "\n",
    "df.groupby(['model_year', 'cylinders']).mean()\n",
    "\n",
    "df.groupby(['model_year', 'cylinders']).mean().columns\n",
    "\n",
    "df.groupby(['model_year', 'cylinders']).mean().index\n",
    "\n",
    "df.groupby('model_year').describe()\n",
    "\n",
    "df.groupby('model_year').describe().transpose()  / df.groupby('model_year').describe().T\n",
    "\n",
    "year_cyl = df.groupby(['model_year', 'cylinders']).mean()  # see the notebook named \"DS_01_03_PreClass - Pandas2 - Usefull Methods\"\n",
    "year_cyl.index.names\n",
    "year_cyl.index.levels \n",
    "year_cyl.loc[70]\n",
    "year_cyl.loc[[70, 82]]\n",
    "year_cyl.index\n",
    "year_cyl.loc[(70, 4)]\n",
    "\n",
    "year_cyl.xs(key=70, level = 'model_year') # cross section\n",
    "\n",
    "df[df['cylinders'].isin([6,8])]\n",
    "df[df['cylinders'].isin([6,8])].groupby(['model_year', 'cylinders']).mean() # filtering + groupby\n",
    "\n",
    "year_cyl.swaplevel()\n",
    "\n",
    "year_cyl.sort_index(level='model_year', ascending=False)\n",
    "\n",
    "df.agg(['std', 'mean'], axis=0)\n",
    "df.agg(['std', 'mean'], axis=0)['mpg']\n",
    "df.agg({'mpg':['max', 'mean'], 'weight':['max', 'mean']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-burlington",
   "metadata": {},
   "source": [
    "### missing data "
   ]
  },
  {
   "cell_type": "raw",
   "id": "downtown-midwest",
   "metadata": {},
   "source": [
    "np.nan       pd.NA       pd.NaT\n",
    "\n",
    ".isin       .isnull      .notnull    \n",
    "\n",
    ".dropna    .fillna      .interpolate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan\n",
    "\n",
    "pd.NA  # NA (\"not available\") missing value indicator.\n",
    "\n",
    "pd.NaT # # (N)ot-(A)-(T)ime, the time equivalent of NaN\n",
    "\n",
    "df.isnull() # boolean\n",
    "\n",
    "df['pre_movie_score'].isnull()\n",
    "df[df['pre_movie_score'].isnull()]\n",
    "\n",
    "df.notnull()\n",
    "\n",
    "df['pre_movie_score'].notnull()\n",
    "df[df['pre_movie_score'].notnull()]\n",
    "\n",
    "df[(df['pre_movie_score'].isnull()) & (df['first_name'].notnull())]\n",
    "\n",
    "df.dropna()\n",
    "\n",
    "df.dropna(thresh = 4)\n",
    "\n",
    "df.dropna(axis=1)\n",
    "\n",
    "df.dropna(axis = 0)\n",
    "\n",
    "df.dropna(subset = ['last_name'])\n",
    "\n",
    "df.fillna('NEW VALUE!')\n",
    "\n",
    "df['pre_movie_score'].fillna(0)\n",
    "df['pre_movie_score'] = df['pre_movie_score'].fillna(0)\n",
    "\n",
    "df['pre_movie_score'].mean()\n",
    "df['pre_movie_score'] = df['pre_movie_score'].mean()\n",
    "\n",
    "df['pre_movie_score'].fillna(df['pre_movie_score'].mean())\n",
    "\n",
    "df.fillna(df.mean())\n",
    "\n",
    "airline_tix = {'first':100, 'business':np.nan, 'economy-plus':50, 'economy':30}\n",
    "ser = pd.Series(airline_tix)\n",
    "ser.interpolate()  #  Fill NaN values using an interpolation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-machinery",
   "metadata": {},
   "source": [
    "### Merging Joining and Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([one, two], axis=1)  # one and two are 2 dataframes\n",
    "\n",
    "pd.concat([one, two], axis=0)\n",
    "\n",
    "two.columns = one.columns\n",
    "pd.concat([one, two], axis = 0)\n",
    "\n",
    "mydf = pd.concat([one, two], axis = 0)\n",
    "mydf.index = range(len(mydf))\n",
    "\n",
    "help(pd.merge)\n",
    "\n",
    "pd.merge(registrations, logins, how='inner', on='name') \n",
    "pd.merge(logins, registrations,  how='inner', on='name')\n",
    "\n",
    "pd.merge(left=registrations, right=logins, how='left', on='name')\n",
    "\n",
    "pd.merge(left=registrations, right=logins, how='right', on='name')\n",
    "\n",
    "pd.merge(registrations, logins, how='outer', on='name')\n",
    "pd.merge(logins, registrations, how='outer', on='name')\n",
    "\n",
    "pd.merge(registrations, logins, left_index=True, right_on='name', how='inner')\n",
    "pd.merge(registrations, logins, how='inner', left_on='reg_name', right_on='name')\n",
    "pd.merge(registrations, logins, how='inner', on='name') # when the names are the same\n",
    "pd.merge(registrations, logins, how='inner', on='name', suffixes=('_reg', '_log'))  # add suffixes if the columns other than the 'name' are the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-tampa",
   "metadata": {},
   "source": [
    "### Text methods for string data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_finance = ['GOOG, APPL, AMZN', 'JPM,BAC,GS']\n",
    "tickers = pd.Series(tech_finance)\n",
    "\n",
    "tickers.str.split(',')\n",
    "\n",
    "tickers.str.split(',', expand = True)\n",
    "\n",
    "messy_names = pd.Series(['andrew  ', \"bo;bo\", \"   claire   \"])\n",
    "\n",
    "messy_names.str.replace(';', '').str.strip().str.capitalize()\n",
    "\n",
    "def cleanup(name):\n",
    "    name = name.replace(\";\",\"\")\n",
    "    name = name.strip()\n",
    "    name = name.capitalize()\n",
    "    return nam\n",
    "\n",
    "messy_names.apply(cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-acoustic",
   "metadata": {},
   "source": [
    "### Time Methods for Date and Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "myyear = 2015\n",
    "mymonth = 1\n",
    "myday = 1\n",
    "myhour = 2\n",
    "mymin = 30\n",
    "mysec = 15\n",
    "mydate = datetime(myyear, mymonth, myday)\n",
    "\n",
    "mydatetime = datetime(myyear, mymonth, myday, myhour, mymin, mysec)\n",
    "\n",
    "mydatetime.year\n",
    "\n",
    "myser = pd.Series(['Nov 3, 1990', '2000-01-01', None])\n",
    "pd.to_datetime(myser)\n",
    "\n",
    "timeser = pd.to_datetime(myser)\n",
    "\n",
    "timeser[0].year\n",
    "\n",
    "obvi_euro_date = '31-12-2000'\n",
    "pd.to_datetime(obvi_euro_date)\n",
    "\n",
    "euro_date = '10-12-2000'\n",
    "pd.to_datetime(euro_date, dayfirst=True) # default False\n",
    "\n",
    "style_date = '12--Dec--2000'\n",
    "pd.to_datetime(style_date, format='%d--%b--%Y')\n",
    "\n",
    "custom_date = '12th of Dec 2000'\n",
    "pd.to_datetime(custom_date)\n",
    "\n",
    "\n",
    "sales = pd.read_csv('RetailSales_BeerWineLiquor.csv')\n",
    "sales['DATE'] # see it's object type\n",
    "sales['DATE'] = pd.to_datetime(sales['DATE'])\n",
    "\n",
    "sales['DATE'][0].year\n",
    "\n",
    "sales.resample(rule = \"A\").mean().head()\n",
    "# TIMES SERIES OFFSET ALIASES--> https://pandas.pydata.org/pandas-doc\n",
    "\n",
    "sales = pd.read_csv('RetailSales_BeerWineLiquor.csv', parse_dates = [0])\n",
    "sales['DATE'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-competition",
   "metadata": {},
   "source": [
    "### Pandas Input and Input Output to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "Reader and Writer of CSV, HTML, Excel and SQL files\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd() # output--> 'C:\\\\Users\\\\hÃ¼seyin'\n",
    "\n",
    "df = pd.read_csv('example.csv')\n",
    "\n",
    "df = pd.read_csv('example.csv', header = None)\n",
    "\n",
    "df = pd.read_csv('example.csv', index_col = 0) # use the zero index column as my actual index in the data frame\n",
    "\n",
    "df.to_csv('C:\\\\Users\\\\hÃ¼seyin\\\\newfile.csv', index = False) # default True\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/World_population\"\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "url = \"view-source_https___en.wikipedia.org_wiki_World_population_\"\n",
    "len(tables) # found 26 tables\n",
    "tables[0]\n",
    "\n",
    "world_topten = tables[0]\n",
    "world_topten = world_topten[\"World population (millions, UN estimates)[15]\"]\n",
    "world_topten = world_topten.drop(11, axis = 0\n",
    "world_topten.columns = ['Country', '2000', '2015', '2030 Est']\n",
    "                                 \n",
    "world_topten.to_html('sample_table.html', index = False)\n",
    "\n",
    "df = pd.read_excel('example_excel_file.xlsx')\n",
    "\n",
    "wb = pd.ExcelFile('example_excel_file.xlsx')                                \n",
    "                                 \n",
    "wb.sheet_names                                \n",
    "                                 \n",
    "excel_sheet_dict = pd.read_excel('example_excel_file.xlsx', sheet_name=None)\n",
    "                                 \n",
    "excel_sheet_dict['Sheet1']\n",
    "                                 \n",
    "excel_sheet_dict1 = pd.read_excel('example_excel_file.xlsx')                                \n",
    "excel_sheet_dict2 = pd.read_excel('example_excel_file.xlsx', sheet_name=['Sheet1'])                                 \n",
    "our_df = excel_sheet_dict2['Sheet1']                                 \n",
    "                                 \n",
    "pip install sqlalchemy\n",
    "                                 \n",
    "https://docs.sqlalchemy.org/en/14/dialects/                                 \n",
    "                                 \n",
    "from sqlalchemy import create_engine                                 \n",
    "                                 \n",
    "temp_db = create_engine('sqlite:///:memory:')    # it is just creates a temporary SQLite database inside of a computers RAM                              \n",
    "                                 \n",
    "pd.DataFrame(data=np.random.randint(low=0, high=100, size=(4,4)), columns = ['a', 'b', 'c', 'd'])                                 \n",
    "                                 \n",
    "df.to_sql(name = 'new_table', con=temp_db)                                 \n",
    "df.to_sql(name = 'new_table', con=temp_db, if_exists='append')                                  \n",
    "                                 \n",
    "result = pd.read_sql_query(sql='SELECT Gender, Country FROM new_table', con=temp_db)                                 \n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-petroleum",
   "metadata": {},
   "source": [
    "### Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-muscle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "digital-array",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_table.quantile(0.25)\n",
    "Q3 = df_table.quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "\n",
    "Q1,Q3 = np.percentile(df[\"table\"],[25,75])\n",
    "Q1, Q3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
